{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Users/jlh/opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/jlh/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/jlh/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/jlh/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /Users/jlh/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=3edc9e65f1ada7fa5e24b31363f0f29d5c047eb286b7129a4d2b4ca674e49a70\n",
      "  Stored in directory: /Users/jlh/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# Update sklearn to prevent version mismatches\n",
    "!pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/jlh/opt/anaconda3/lib/python3.8/site-packages (0.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "# install joblib. This will be used to save your model. \n",
    "# Restart your kernel after installing \n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cumulative.csv\")\n",
    "# Drop the null columns where all values are null\n",
    "df = df.drop(columns=[\"kepler_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the null columns where all values are null\n",
    "df = df.dropna(axis='columns', how='all')\n",
    "# Drop the null rows\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"rowid\", \"kepid\", \"koi_pdisposition\", \"koi_score\", \"koi_tce_delivname\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select your features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features. This will also be used as your x values.\n",
    "#selected_features = df[['koi_disposition', 'koi_period', 'koi_time0bk', 'koi_impact', 'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol', 'koi_model_snr', 'koi_steff', 'koi_slogg', 'koi_srad']]\n",
    "#selected_features\n",
    "X = df.drop(columns=[\"koi_disposition\", \"kepoi_name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped features based on importance\n",
    "X = df.drop(columns=[\"koi_disposition\", \"kepoi_name\", \"koi_srad_err1\", 'koi_teq', 'koi_period_err2', 'koi_time0bk_err2', 'koi_insol_err2', 'koi_depth_err2', 'koi_srad', 'koi_depth_err1', 'koi_tce_plnt_num', 'koi_prad_err2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Train Test Split\n",
    "\n",
    "Use `koi_disposition` for the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose our y variable\n",
    "y = df[\"koi_disposition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our train_test_split dependency\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7556         CANDIDATE\n",
       "2272         CANDIDATE\n",
       "5020         CONFIRMED\n",
       "7434    FALSE POSITIVE\n",
       "5058         CANDIDATE\n",
       "Name: koi_disposition, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Scale the data using the MinMaxScaler and perform some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "label_encoder.fit(y_test)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "hot_y_train = to_categorical(encoded_y_train)\n",
    "hot_y_test = to_categorical(encoded_y_test)\n",
    "hot_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8416196822142491"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the accuracy of the machine learning model\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train_scaled, hot_y_train)\n",
    "clf.score(X_test_scaled, hot_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model does not exceed an 85% accuracy rating. How can we improve this model?\n",
    "model1 = clf\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.20474763266167573, 'koi_fpflag_ss'),\n",
       " (0.18787617607797952, 'koi_fpflag_co'),\n",
       " (0.15020447861738895, 'koi_fpflag_nt'),\n",
       " (0.12373917060698204, 'koi_model_snr'),\n",
       " (0.031078683680212455, 'koi_fpflag_ec'),\n",
       " (0.027336663810552966, 'koi_impact'),\n",
       " (0.01872012300870629, 'koi_duration'),\n",
       " (0.018495236122294294, 'koi_prad_err1'),\n",
       " (0.01629322994792188, 'koi_prad'),\n",
       " (0.014994753968054317, 'dec'),\n",
       " (0.014734827502718347, 'koi_kepmag'),\n",
       " (0.014675173027460098, 'koi_slogg_err2'),\n",
       " (0.013196059100492258, 'koi_time0bk'),\n",
       " (0.012795742056719106, 'koi_slogg'),\n",
       " (0.012779439720260388, 'ra'),\n",
       " (0.01225171616977904, 'koi_depth'),\n",
       " (0.011916029016619423, 'koi_srad_err2'),\n",
       " (0.011038420216855664, 'koi_steff'),\n",
       " (0.010591811806552229, 'koi_impact_err1'),\n",
       " (0.010416454969314556, 'koi_period'),\n",
       " (0.010136178473875409, 'koi_steff_err1'),\n",
       " (0.009468561095833564, 'koi_impact_err2'),\n",
       " (0.00905404626918839, 'koi_period_err1'),\n",
       " (0.008900239474351597, 'koi_insol_err1'),\n",
       " (0.008773190206134237, 'koi_steff_err2'),\n",
       " (0.008562253915078463, 'koi_duration_err1'),\n",
       " (0.008283902203968959, 'koi_time0bk_err1'),\n",
       " (0.0074148104458941376, 'koi_slogg_err1'),\n",
       " (0.0070452529762860885, 'koi_insol'),\n",
       " (0.004479742850849635, 'koi_duration_err2')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the columns by their feature importance. By culling some of the less important columns we can reduce the work done by our computer\n",
    "sorted(zip(clf.feature_importances_, feature_names), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the least important columns. Un comment them and paste them as our X variable\n",
    "#X = df.drop(columns=[\"koi_disposition\", \"kepoi_name\", \"koi_srad_err1\", 'koi_teq', 'koi_period_err2', 'koi_time0bk_err2', 'koi_insol_err2', 'koi_depth_err2', 'koi_srad', 'koi_depth_err1', 'koi_tce_plnt_num', 'koi_prad_err2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase the accuracy of the model, we'll need to tune the hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlh/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': None,\n",
       " 'class_weight': None,\n",
       " 'criterion': None,\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': None,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': None,\n",
       " 'min_samples_split': None,\n",
       " 'min_weight_fraction_leaf': None,\n",
       " 'presort': None,\n",
       " 'random_state': None,\n",
       " 'splitter': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier.get_params(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [5, 7, 10, 15, 25, 40, 65, 95, 150]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree hyper parameters include the parameters below. We're going to use GridSearchCV to choose the best ones for us\n",
    "scoring = {'AUC': 'roc_auc'}\n",
    "param_grid = {'criterion':['gini', 'entropy'], 'max_depth':[5, 7, 10, 15, 25, 40, 65, 95, 150]}\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "gs.fit(X_train_scaled, hot_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 7}\n",
      "0.8650041238768822\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gs.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8754484879548949"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Success! We've improved our accuracy from 84.1% to 87.5%, 3.5% increase. \n",
    "gs.score(X_test_scaled, hot_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DecisionTree.sav']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save your model by updating \"your_name\" with your name\n",
    "# and \"your_model\" with your model variable\n",
    "# be sure to turn this in to BCS\n",
    "# if joblib fails to import, try running the command to install in terminal/git-bash\n",
    "#https://stackoverflow.com/questions/38709690/scikit-learn-using-gridsearchcv-on-decisiontreeclassifier\n",
    "import joblib\n",
    "filename = 'DecisionTree.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
